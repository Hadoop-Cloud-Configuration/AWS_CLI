1. List the instrumented HotSpot Java Virtual Machines(JVM) on the target system:
jps

2. Run pi example:

Enter hadoop user: su hdfs


hadoop jar hadoop-examples-1.2.1.jar pi 10 1000000

groupadd hdfs
usermod -a -G hdfs root
hadoop fs -chmod 777 /user 


# one solution
sudo -i enter root

find . -name "hdfs-site.xml"


yarn jar hadoop-mapreduce-examples-2.7.1.2.3.2.0-2950.jar teragen 1000 output

yarn jar hadoop-mapreduce-examples-2.7.1.2.3.2.0-2950.jar pi 10 1000

hadoop-mapreduce-examples-2.7.1.2.3.2.0-2950.jar

su hdfs

add in conf/hdfs-site.xml

  <property>
    <name>dfs.permissions</name>
    <value>false</value>
  </property>

examples directory: /usr/hdp/2.3.2.0-2950/hadoop-mapreduce

3.check hadoop version:
hadoop version

4.List files in Hadoop File System(HDFS):
hadoop dfs -ls

5.Copy data file to Hadoop File System:
hadoop dfs -copyFromLocal #source-destination #target-destination

6.Run word count example
hadoop jar /home/hadoop/hadoop/hadoop-0.19.2-examples.jar wordcount dft dft-output

hadoop dfs -ls

hadoop dfs -ls dft-output

hadoop dfs -cat dft-output/part-00000 | less

hdfs dfs -ls

If we wanted to copy the output file to our local storage (remember, the output is automatically created in the HDFS world, and we have to copy the data from there to our file system to work on it):

cd ~/352/dft

hadoop dfs -copyToLocal dft-output/part-00000 . hadoop@hadoop1:~/352/dft$ ls  4300.txt part-00000 

hadoop dfs -rmr dft-output 
